{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pipe line \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read train data\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# Look at the shape of the data\n",
    "print('Train shape:', train.shape)\n",
    "\n",
    "# Look at the head() of the data\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "테스트는 train보다 열이 하나 적어야함(target), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test data\n",
    "test = pandas.read_csv('test.csv')\n",
    "\n",
    "# Print train and test columns\n",
    "print('Train columns:', train.columns.tolist())\n",
    "print('Test columns:', test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the sample submission file\n",
    "sample_submission = pandas.read_csv('sample_submission.csv')\n",
    "\n",
    "# Look at the head() of the sample submission\n",
    "print(sample_submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Read the train data\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# Create a Random Forest object\n",
    "rf = sklearn.RandomForestRegressor()\n",
    "\n",
    "# Train a model\n",
    "rf.fit(X=train[['store', 'item']], y=train['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test and sample submission data\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Show the head() of the sample_submission\n",
    "print(sample_submission.head())\n",
    "\n",
    "# Get predictions for the test set\n",
    "test['sales'] = rf.predict(test[['store', 'item']])\n",
    "\n",
    "# Write test predictions using the sample_submission format\n",
    "test[[\"id\", \"sales\"]].to_csv('kaggle_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create DMatrix on train data\n",
    "dtrain = xgb.DMatrix(data=train[['store', 'item']],\n",
    "                     label=train['sales'])\n",
    "\n",
    "# Define xgboost parameters\n",
    "params = {'objective': 'reg:linear',\n",
    "          'max_depth': 2,\n",
    "          'silent': 1}\n",
    "\n",
    "# Train xgboost model\n",
    "xg_depth_2 = xgb.train(params=params, dtrain=dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DMatrix on train data\n",
    "dtrain = xgb.DMatrix(data=train[['store', 'item']],\n",
    "                     label=train['sales'])\n",
    "\n",
    "# Define xgboost parameters\n",
    "params = {'objective': 'reg:linear',\n",
    "          'max_depth': 8,\n",
    "          'silent': 1}\n",
    "\n",
    "# Train xgboost model\n",
    "xg_depth_8 = xgb.train(params=params, dtrain=dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DMatrix on train data\n",
    "dtrain = xgb.DMatrix(data=train[['store', 'item']],\n",
    "                     label=train['sales'])\n",
    "\n",
    "# Define xgboost parameters\n",
    "params = {'objective': 'reg:linear',\n",
    "          'max_depth': 15,\n",
    "          'silent': 1}\n",
    "\n",
    "# Train xgboost model\n",
    "xg_depth_15 = xgb.train(params=params, dtrain=dtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn에 있는 score 함수 쓰기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "dtrain = xgb.DMatrix(data=train[['store', 'item']])\n",
    "dtest = xgb.DMatrix(data=test[['store', 'item']])\n",
    "\n",
    "# For each of 3 trained models\n",
    "for model in [xg_depth_2, xg_depth_8, xg_depth_15]:\n",
    "    # Make predictions\n",
    "    train_pred = model.predict(dtrain)     \n",
    "    test_pred = model.predict(dtest)          \n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse_train = mean_squared_error(train['sales'], train_pred)                  \n",
    "    mse_test = mean_squared_error(test['sales'], test_pred)\n",
    "    print('MSE Train: {:.3f}. MSE Test: {:.3f}'.format(mse_train, mse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score 함수 직접 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Import MSE from sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "# Define your own MSE function\n",
    "def own_rms(y_true, y_pred):\n",
    "  \t# Raise differences to the power of 2\n",
    "    squares = np.power(y_true - y_pred, 2)\n",
    "    # Find mean over all observations\n",
    "    err = np.mean(squares)\n",
    "    rmse = np.sqrt(err)\n",
    "    return err\n",
    "\n",
    "## sklearn 사용 \n",
    "print('Sklearn MSE: {:.5f}. '.format(mean_squared_error(y_regression_true, y_regression_pred) ** 0.5))\n",
    "\n",
    "## 직접 \n",
    "print('Your MSE: {:.5f}. '.format(own_mse(y_regression_true, y_regression_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Import log_loss from sklearn\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Define your own LogLoss function\n",
    "def own_logloss(y_true, prob_pred):\n",
    "  \t# Find loss for each observation\n",
    "    terms = y_true * np.log(prob_pred) + (1 - y_true) * np.log(1 - prob_pred)\n",
    "    # Find mean over all observations\n",
    "    err = np.mean(terms) \n",
    "    return -err\n",
    "\n",
    "print('Sklearn LogLoss: {:.5f}'.format(log_loss(y_classification_true, y_classification_pred)))\n",
    "print('Your LogLoss: {:.5f}'.format(own_logloss(y_classification_true, y_classification_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eda 시작 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapes of train and test data\n",
    "print('Train shape:', train.shape)\n",
    "print('Test shape:', test.shape)\n",
    "\n",
    "# Train head()\n",
    "print(train.head())\n",
    "\n",
    "# Describe the target variable\n",
    "print(train.fare_amount.describe())\n",
    "\n",
    "# Train distribution of passengers within rides\n",
    "print(train.passenger_count.vaalue_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ride distance\n",
    "train['distance_km'] = haversine_distance(train)\n",
    "\n",
    "# Draw a scatterplot\n",
    "plt.scatter(x=train[\"fare_amount\"], y=train[\"distance_km\"], alpha=0.5)\n",
    "plt.xlabel('Fare amount')\n",
    "plt.ylabel('Distance, km')\n",
    "plt.title('Fare amount based on the distance')\n",
    "\n",
    "# Limit on the distance\n",
    "plt.ylim(0, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dateatime에서 시간만 pickup하는거 주의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hour feature\n",
    "train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\n",
    "train['hour'] = train.pickup_datetime.dt.hour\n",
    "\n",
    "# Find median fare_amount for each hour\n",
    "hour_price = train.groupby('hour', as_index=False)['fare_amount'].median()\n",
    "\n",
    "# Plot the line plot\n",
    "plt.plot(hour_price[\"hour\"], hour_price[\"fare_amount\"], marker='o')\n",
    "plt.xlabel('Hour of the day')\n",
    "plt.ylabel('Median fare amount')\n",
    "plt.title('Fare amount based on day time')\n",
    "plt.xticks(range(24))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기선 시간을 뽑아냈는데. 그냥 범주형으로 낮인가, 밤인가 구별하는것도 좋은방법\n",
    "\"is_night\" True, False로 넣는다던가.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create a KFold object\n",
    "kf = KFold(n_splits= 3 , shuffle=True, random_state=123)\n",
    "\n",
    "# Loop through each split\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # Obtain training and testing folds\n",
    "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    print('Fold: {}'.format(fold))\n",
    "    print('CV train shape: {}'.format(cv_train.shape))\n",
    "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create a StratifiedKFold object\n",
    "str_kf = StratifiedKFold(n_splits= 3, shuffle= True , random_state=123)\n",
    "\n",
    "# Loop through each split\n",
    "fold = 0\n",
    "for train_index, test_index in str_kf.split(train, train['interest_level']):\n",
    "    # Obtain training and testing folds\n",
    "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    print('Fold: {}'.format(fold))\n",
    "    print('CV train shape: {}'.format(cv_train.shape))\n",
    "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리 해커톤도 time_series라 밑의 time_kfold를 사용해야합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TimeSeriesSplit object\n",
    "time_kfold = TimeSeriesSplit(n_splits= 3)\n",
    "\n",
    "# Sort train data by date\n",
    "train = train.sort_values(by = 'date')\n",
    "\n",
    "# Iterate through each split\n",
    "fold = 0\n",
    "for train_index, test_index in time_kfold.split(train):\n",
    "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    \n",
    "    print('Fold :', fold)\n",
    "    print('Train date range: from {} to {}'.format(cv_train.date.min(), cv_train.date.max()))\n",
    "    print('Test date range: from {} to {}\\n'.format(cv_test.date.min(), cv_test.date.max()))\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import numpy as np\n",
    "\n",
    "# Sort train data by date\n",
    "train = train.sort_values('date')\n",
    "\n",
    "# Initialize 3-fold time cross-validation\n",
    "kf = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# Get MSE scores for each cross-validation split\n",
    "mse_scores = get_fold_mse(train, kf)\n",
    "\n",
    "print('MSE by fold: {}'.format(mse_scores))\n",
    "\n",
    "print('Overall validation MSE: {:.5f}'.format(np.mean(mse_scores) + np.std(mse_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data featuring , 산수를 통해 새 변수 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_kfold_rmse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9c152036c711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Look at the initial RMSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RMSE before feature engineering:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_kfold_rmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Find the total area of the house\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TotalArea'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TotalBsmtSF'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FirstFlrSF'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SecondFlrSF'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_kfold_rmse' is not defined"
     ]
    }
   ],
   "source": [
    "# Look at the initial RMSE\n",
    "print('RMSE before feature engineering:', get_kfold_rmse(train))\n",
    "\n",
    "# Find the total area of the house\n",
    "train['TotalArea'] = train['TotalBsmtSF'] + train['FirstFlrSF'] + train['SecondFlrSF']\n",
    "print('RMSE with total area:', get_kfold_rmse(train))\n",
    "\n",
    "# Find the area of the garden\n",
    "train['GardenArea'] = train['LotArea'] - train['FirstFlrSF']\n",
    "print('RMSE with garden area:', get_kfold_rmse(train))\n",
    "\n",
    "# Find total number of bathrooms\n",
    "train['TotalBath'] = train['FULLBath']  + train['HalfBath']\n",
    "print('RMSE with number of bathrooms:', get_kfold_rmse(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data featuring 시간 바꾸기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test together\n",
    "taxi = pd.concat([train, test])\n",
    "\n",
    "# Convert pickup date to datetime object\n",
    "taxi['pickup_datetime'] = pd.to_datetime(taxi['pickup_datetime'])\n",
    "\n",
    "# Create a day of week feature\n",
    "taxi['dayofweek'] = taxi['pickup_datetime'].dt.dayofweek\n",
    "\n",
    "# Create an hour feature\n",
    "taxi['hour'] = taxi['pickup_datetime'].dt.hour\n",
    "\n",
    "# Split back into train and test\n",
    "new_train = taxi[taxi['id'].isin(train['id'])]\n",
    "new_test = taxi[taxi['id'].isin(test['id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label 코딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test together\n",
    "houses = pd.concat([train, test])\n",
    "\n",
    "# Label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Create new features\n",
    "houses['RoofStyle_enc'] = le.fit_transform(houses['RoofStyle'])\n",
    "houses['CentralAir_enc'] = le.fit_transform(houses['CentralAir'])\n",
    "\n",
    "# Look at new features\n",
    "print(houses[['RoofStyle', 'RoofStyle_enc', 'CentralAir', 'CentralAir_enc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test together\n",
    "houses = pd.concat([train, test])\n",
    "\n",
    "# Look at feature distributions\n",
    "print(houses['RoofStyle'].value_counts(), '\\n')\n",
    "print(houses['CentralAir'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test together\n",
    "houses = pd.concat([train, test])\n",
    "\n",
    "# Label encode binary 'CentralAir' feature\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "houses['CentralAir_enc'] = le.fit_transform(houses['CentralAir'])\n",
    "\n",
    "# Create One-Hot encoded features\n",
    "ohe = pd.get_dummies(houses['RoofStyle'], prefix='RoofStyle')\n",
    "\n",
    "# Concatenate OHE features to houses\n",
    "houses = pd.concat([houses, ohe], axis=1)\n",
    "\n",
    "# Look at OHE features\n",
    "print(houses[[col for col in houses.columns if 'RoofStyle' in col]].head(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
